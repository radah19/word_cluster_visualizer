{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38fc4f5",
   "metadata": {},
   "source": [
    "### Overview\n",
    "This script was originally a python script turned Jupyter notebook - it gives various configurations & choices for modelling cluster visualizations with or without the generated cluster data (it's recommended you just use the already existing cluster data in `~/Data/pkl_clusters`).\n",
    "\n",
    "The generated visualizations run on PyVis and often can have poor performance due to the size of data alongside PyVis simulating physics. I haven't really found a good way to get a good arrangement of the PyVis graph without enabling physics, so often what I do is upon opening the project I enable physics for a bit, then disable it after a few minutes.\n",
    "\n",
    "This is definitely not the best program so please feel free to rectify issues you may find in it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0adf80",
   "metadata": {},
   "source": [
    "#### Setup/Requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install nltk\n",
    "%pip install pyspellchecker\n",
    "%pip install pyvis\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95eae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\brain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "from spellchecker import SpellChecker\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words as nltk_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af3b358",
   "metadata": {},
   "source": [
    "#### Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9530d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vars\n",
    "lg_doc_lt = pd.read_pickle('../../Data/lookup_tables/doc_lookup_table')\n",
    "sm_doc_lt = pd.read_pickle('../../Data/lookup_tables/sm_doc_lookup_table')\n",
    "vs_doc_lt = pd.read_pickle('../../Data/lookup_tables/vs_doc_lookup_table')\n",
    "\n",
    "cluster_distances = [\n",
    "    0.04, 0.059, 0.095, 0.136, 0.188, 0.231, 0.043, 0.062, \n",
    "    0.1, 0.143, 0.19, 0.235, 0.045, 0.067, 0.105, 0.154, \n",
    "    0.2, 0.238, 0.048, 0.071, 0.111, 0.158, 0.211, 0.241, \n",
    "    0.05, 0.077, 0.118, 0.167, 0.214, 0.25, 0.053, 0.083, \n",
    "    0.125, 0.176, 0.222, 0.056, 0.091, 0.133, 0.182, 0.227\n",
    "]\n",
    "\n",
    "# Pyvis Graph Config\n",
    "net = Network(select_menu=True, cdn_resources='remote')\n",
    "net.show_buttons(filter_=['physics'])\n",
    "\n",
    "def pyvisSaveGraph(name: str):\n",
    "    # Disable Physics on Nodes\n",
    "    net.toggle_physics(False)\n",
    "\n",
    "    # Generate Graph with UTF-8 Encoding\n",
    "    html = net.generate_html()\n",
    "    with open(f\"cluster_visualizations/{name}.html\", mode='w', encoding='utf-8') as fp:\n",
    "        fp.write(html)\n",
    "    print(f\"Done! Look for cluster_visualizations/{name}.html\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a40e00",
   "metadata": {},
   "source": [
    "##### Word Frequency Visualization\n",
    "Given a threshold size and a size set for the document list to analyze, generates associations based off levensthein distances between words available in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fd82d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeWordFreqData(threshold: int, doc_lt: dict, distance: int):\n",
    "    timer = 0\n",
    "\n",
    "    # Count Word Frequencies, then filter words not meeting a threshold\n",
    "    all_words_freqs = {}\n",
    "\n",
    "    timer = time.time()\n",
    "\n",
    "    for doc, doc_val in tqdm(doc_lt.items(), desc=\"Counting Frequencies of all Words across documents\".ljust(65)):\n",
    "        for word in doc_val:\n",
    "            # Prevent Nulls from being added\n",
    "            if word == None:\n",
    "                continue       \n",
    "\n",
    "            # Count up times used across documents\n",
    "            if word not in all_words_freqs:\n",
    "                all_words_freqs[word] = 0\n",
    "            all_words_freqs[word] += 1\n",
    "\n",
    "    print(\"\\tExecution time: \", time.time() - timer, \" seconds\")\n",
    "\n",
    "    # Filter by Threshold Value\n",
    "    timer = time.time()\n",
    "\n",
    "    word_freqs = {}\n",
    "    for word, word_freq in tqdm(all_words_freqs.items(), desc=\"Filtering words with frequencies below threshold\".ljust(65)):\n",
    "        if word_freq >= threshold:\n",
    "            word_freqs[word] = word_freq\n",
    "    print(\"\\tExecution time: \", time.time() - timer, \" seconds\")\n",
    "\n",
    "    # Match Similar Words based on Levenshtein Distance\n",
    "    root_groups = {}\n",
    "    corrected_groups = {}\n",
    "\n",
    "    spell = None\n",
    "    if distance == 1:\n",
    "        spell = SpellChecker(distance=1)\n",
    "    else:\n",
    "        spell = SpellChecker()\n",
    "\n",
    "    timer = time.time()\n",
    "\n",
    "    for word, word_freq in tqdm(word_freqs.items(), desc=\"Spell Checking words via Levensthein Algorithm & Grouping them\".ljust(65)):\n",
    "        # Create source groups for grouping nodes later\n",
    "        if word not in corrected_groups:\n",
    "            corrected_word = spell.correction(word)\n",
    "            source_word = corrected_word if corrected_word != None else word\n",
    "            corrected_groups[word] = source_word\n",
    "        else:\n",
    "            source_word = corrected_groups[word]  \n",
    "\n",
    "        # Associate groups\n",
    "        if source_word not in root_groups:\n",
    "            root_groups[source_word] = []\n",
    "        \n",
    "        if word not in root_groups[source_word]:\n",
    "            root_groups[source_word].append(word)\n",
    "    \n",
    "    print(\"\\tExecution time: \", time.time() - timer, \" seconds\")\n",
    "\n",
    "    # Create Visual Graph\n",
    "    timer = time.time()\n",
    "\n",
    "    for word, word_freq in tqdm(word_freqs.items(), desc=\"Generating Graph...\".ljust(65)):\n",
    "        _group = corrected_groups[word]  \n",
    "\n",
    "        if len(root_groups[_group]) > 1:\n",
    "            # Add Node to diagram\n",
    "            net.add_node(\n",
    "                word, \n",
    "                size=max(min(word_freq / 2, 30), 3),  \n",
    "                label=f\"{word}\\n({word_freq})\",\n",
    "                group=_group\n",
    "                )\n",
    "\n",
    "            # Add edges to common node to group them together\n",
    "            if root_groups[_group][0] != word:\n",
    "                net.add_edge(\n",
    "                    word,\n",
    "                    root_groups[_group][0],\n",
    "                    width=0\n",
    "                )\n",
    "    \n",
    "    print(\"\\tExecution time: \", time.time() - timer, \" seconds\")\n",
    "\n",
    "    pyvisSaveGraph(f\"word_freq_diagram_{threshold}_{distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a00b9",
   "metadata": {},
   "source": [
    "Modify Variables below to play with function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38f56c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Frequencies of all Words across documents               : 42it [00:00, 4570.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecution time:  0.011195182800292969  seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering words with frequencies below threshold                 : 100%|██████████| 4482/4482 [00:00<00:00, 720290.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecution time:  0.007234096527099609  seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spell Checking words via Levensthein Algorithm & Grouping them   : 100%|██████████| 298/298 [00:00<00:00, 406.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecution time:  0.744495153427124  seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Graph...                                              : 100%|██████████| 298/298 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecution time:  0.002257823944091797  seconds\n",
      "Done! Look for cluster_visualizations/word_freq_diagram_5_2.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Minimum threshold of occurences for a word to appear on graph\n",
    "threshold = 5 \n",
    "# Word sets to look from (choices: vs_doc_lt, sm_doc_lt, lg_doc_lt)\n",
    "doc_choice = sm_doc_lt\n",
    "# Spellchecking Distance (choices: 1 (levensthein distance of 1), 2 (levensthein distance of 2))\n",
    "distance = 2\n",
    "\n",
    "visualizeWordFreqData(threshold, doc_choice, distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc94a3",
   "metadata": {},
   "source": [
    "##### Word Cluster Visualization\n",
    "Given a distance value to reference from the already ran cluster component algorithm, generates a basic cluster component diagram showing all clusters and the words in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da1c16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeClusterCompData(distance: float, min_threshold: int, max_threshold: int):\n",
    "    timer = 0\n",
    "    initial_ls = pd.read_pickle(f'../../Data/pkl_clusters/connected_comps_{distance}')\n",
    "    cluster_ls = []\n",
    "\n",
    "    # Filter Cluster Comp List by thresholds\n",
    "    if min_threshold != -1 or max_threshold != -1:\n",
    "        timer = time.time()\n",
    "        for ls in tqdm(initial_ls, desc=\"Compiling cluster list from data: \".ljust(65)):\n",
    "            if len(ls) >= min_threshold and (max_threshold == -1 or len(ls) <= max_threshold):\n",
    "                cluster_ls.append(ls)\n",
    "        print(\"\\tExecution time: \", time.time() - timer, \" seconds\")\n",
    "    else:\n",
    "        cluster_ls = initial_ls\n",
    "\n",
    "    # Generate Graph\n",
    "    timer = time.time()\n",
    "    word_set = nltk_words.words()\n",
    "\n",
    "    for ls in tqdm(cluster_ls, desc=\"Generating Graph & Pairing words: \".ljust(65)):\n",
    "        # Edge case in case there's only one word\n",
    "        if len(ls) <= 1:\n",
    "            net.add_node(\n",
    "                ls[0], \n",
    "                size=2,  \n",
    "                label=f\"{ls[0]}\",\n",
    "                group=ls[0]\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        # Identify Correct Word to link the words to, or at least the closest\n",
    "        correct_word = ls[0]\n",
    "\n",
    "        for word in ls: \n",
    "            if word in word_set:\n",
    "                correct_word = word\n",
    "                break\n",
    "\n",
    "        net.add_node(\n",
    "            correct_word, \n",
    "            size=9, \n",
    "            label=f\"{correct_word}\",\n",
    "            group=correct_word\n",
    "        )\n",
    "        \n",
    "        for word in ls: \n",
    "            if word != correct_word:\n",
    "                net.add_node(\n",
    "                    word, \n",
    "                    size=6, \n",
    "                    label=f\"{word}\",\n",
    "                    group=correct_word\n",
    "                )\n",
    "    \n",
    "                net.add_edge(\n",
    "                    word,\n",
    "                    correct_word,\n",
    "                    width=0\n",
    "                )\n",
    "\n",
    "    print(\"\\tExecution time: \", time.time() - timer, \" seconds\")\n",
    "\n",
    "    pyvisSaveGraph(f\"cluster_comp_diagram_{distance}_{min_threshold}_{max_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf73b049",
   "metadata": {},
   "source": [
    "Modify Variables below to play with function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdb8b80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling cluster list from data:                                : 100%|██████████| 100380/100380 [00:00<00:00, 4488914.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecution time:  0.023319244384765625  seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Graph & Pairing words:                                : 100%|██████████| 2688/2688 [01:20<00:00, 33.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecution time:  80.96397066116333  seconds\n",
      "Done! Look for cluster_visualizations/cluster_comp_diagram_0.211_5_20.html\n"
     ]
    }
   ],
   "source": [
    "# Distance value for clustering comp algorithm to reference\n",
    "\n",
    "# Choices:      0.04, 0.059, 0.095, 0.136, 0.188, 0.231, 0.043, 0.062, \n",
    "#               0.1, 0.143, 0.19, 0.235, 0.045, 0.067, 0.105, 0.154, \n",
    "#               0.2, 0.238, 0.048, 0.071, 0.111, 0.158, 0.211, 0.241, \n",
    "#               0.05, 0.077, 0.118, 0.167, 0.214, 0.25, 0.053, 0.083, \n",
    "#               0.125, 0.176, 0.222, 0.056, 0.091, 0.133, 0.182, 0.227\n",
    "distance = 0.211\n",
    "\n",
    "# Clusters with less words than this threshold are not included in visualization\n",
    "min_threshold = 5\n",
    "# Clusters with more words than this threshold are not included in visualization\n",
    "max_threshold = 20\n",
    "\n",
    "visualizeClusterCompData(distance, min_threshold, max_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b4a1a",
   "metadata": {},
   "source": [
    "##### Word Cluster with Frequency Visualizer\n",
    "Given a distance value to reference from the already ran cluster component algorithm, generates a basic cluster component diagram showing all clusters and the words in them. Includes also their frequency based off a document lookup table set.\n",
    "\n",
    "Personally this one isn't that useful... It's just a blend of the previous two algorithms without many benefits. Stick with the Cluster Component Visualization if this may not be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a5360da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeClusterCompFreqData(distance: float, freq_threshold: int, doc_lt: dict, min_cluster_threshold: int, max_cluster_threshold: int):\n",
    "    timer = 0\n",
    "\n",
    "    # Count Word Frequencies, then filter words not meeting a threshold\n",
    "    all_words_freqs = {}\n",
    "    timer = time.time()\n",
    "    for doc, doc_val in tqdm(doc_lt.items(), desc=\"Counting Frequencies of all Words across documents\".ljust(65)):\n",
    "        for word in doc_val:\n",
    "            # Prevent Nulls from being added\n",
    "            if word == None:\n",
    "                continue       \n",
    "\n",
    "            # Count up times used across documents\n",
    "            if word not in all_words_freqs:\n",
    "                all_words_freqs[word] = 0\n",
    "            all_words_freqs[word] += 1\n",
    "    print(\"\\tExecution time: \", time.time() - timer, \" seconds\")\n",
    "\n",
    "    # Filter by Threshold Value\n",
    "    timer = time.time()\n",
    "    word_freqs = {}\n",
    "    for word, word_freq in tqdm(all_words_freqs.items(), desc=\"Filtering words with frequencies below threshold\".ljust(65)):\n",
    "        if word_freq >= freq_threshold:\n",
    "            word_freqs[word] = word_freq\n",
    "    print(\"\\tExecution time: \", time.time() - timer, \" seconds\")\n",
    "\n",
    "    # Merge pickled information into one list\n",
    "    initial_ls = pd.read_pickle(f'../../Data/pkl_clusters/connected_comps_{distance}')\n",
    "    full_cluster_ls = []\n",
    "\n",
    "    # Filter Cluster Comp List by thresholds\n",
    "    if min_cluster_threshold != -1 or max_cluster_threshold != -1:\n",
    "        timer = time.time()\n",
    "        for ls in tqdm(initial_ls, desc=\"Compiling cluster list from data: \".ljust(65)):\n",
    "            if len(ls) >= min_cluster_threshold and (max_cluster_threshold == -1 or len(ls) <= max_cluster_threshold):\n",
    "                full_cluster_ls.append(ls)\n",
    "        print(\"\\tExecution time: \", time.time() - timer, \" seconds\")\n",
    "    else:\n",
    "        full_cluster_ls = initial_ls\n",
    "\n",
    "    # Filter by Words with recorded word frequencies\n",
    "    timer = time.time()\n",
    "    final_cluster_ls = []\n",
    "    for ls in tqdm(full_cluster_ls, desc=\"Culling clusters with low frequencies: \".ljust(65)):\n",
    "        # Remove words from each list that did not meet the word frequency threshold\n",
    "        for word in ls[:]: # Iterate through copy of list\n",
    "            if word not in word_freqs:\n",
    "                ls.remove(word)\n",
    "        # If every word met requirements, then yay!\n",
    "        if len(ls) > 1:\n",
    "            final_cluster_ls.append(ls)\n",
    "    print(\"\\tExecution time: \", time.time() - timer, \" seconds\")\n",
    "\n",
    "    # Now Perform Clustering Algorithm & Generate Graph\n",
    "    timer = time.time()\n",
    "    word_set = nltk_words.words()\n",
    "\n",
    "    for ls in tqdm(final_cluster_ls, desc=\"Generating Graph & Pairing words: \".ljust(65)):\n",
    "        # print(\"List I'm looking at !! \", ls)\n",
    "        # Edge case in case there's only one word\n",
    "        if len(ls) <= 1:\n",
    "            net.add_node(\n",
    "                word, \n",
    "                size=2,  \n",
    "                label=f\"{word}\\n({word_freqs[word] if word in word_freqs else 'BROKEN'})\",\n",
    "                group=word\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        # Identify Correct Word to link the words to, or at least the closest\n",
    "        correct_word = ls[0]\n",
    "\n",
    "        for word in ls: \n",
    "            if word in word_set:\n",
    "                correct_word = word\n",
    "                break\n",
    "\n",
    "        net.add_node(\n",
    "            correct_word, \n",
    "            size=5, \n",
    "            label=f\"{correct_word}\\n({word_freqs[correct_word] if correct_word in word_freqs else 'BROKEN'})\",\n",
    "            group=correct_word\n",
    "            )\n",
    "        \n",
    "        for word in ls: \n",
    "            if word != correct_word:\n",
    "                net.add_node(\n",
    "                    word, \n",
    "                    size=2, \n",
    "                    label=f\"{word}\\n({word_freqs[word] if word in word_freqs else 'BROKEN'})\",\n",
    "                    group=correct_word\n",
    "                )\n",
    "    \n",
    "                net.add_edge(\n",
    "                    word,\n",
    "                    correct_word,\n",
    "                    width=0\n",
    "                )\n",
    "    print(\"\\tExecution time: \", time.time() - timer, \" seconds\")\n",
    "\n",
    "    pyvisSaveGraph(f\"cluster_comp_freq_diagram_{distance}_{freq_threshold}_{min_cluster_threshold}_{max_cluster_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71525f",
   "metadata": {},
   "source": [
    "Modify Variables below to play with function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e982453c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Frequencies of all Words across documents               : 100%|██████████| 1466/1466 [00:00<00:00, 9737.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecution time:  0.1524655818939209  seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering words with frequencies below threshold                 : 100%|██████████| 146393/146393 [00:00<00:00, 4338605.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecution time:  0.03374195098876953  seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling cluster list from data:                                : 100%|██████████| 100380/100380 [00:00<00:00, 3028297.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecution time:  0.033147335052490234  seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Culling clusters with low frequencies:                           : 100%|██████████| 2688/2688 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecution time:  0.0  seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Graph & Pairing words:                                : 100%|██████████| 336/336 [00:02<00:00, 116.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecution time:  2.9471027851104736  seconds\n",
      "Done! Look for cluster_visualizations/cluster_comp_freq_diagram_0.211_10_5_20.html\n"
     ]
    }
   ],
   "source": [
    "# Distance value for clustering comp algorithm to reference\n",
    "\n",
    "# Choices:      0.04, 0.059, 0.095, 0.136, 0.188, 0.231, 0.043, 0.062, \n",
    "#               0.1, 0.143, 0.19, 0.235, 0.045, 0.067, 0.105, 0.154, \n",
    "#               0.2, 0.238, 0.048, 0.071, 0.111, 0.158, 0.211, 0.241, \n",
    "#               0.05, 0.077, 0.118, 0.167, 0.214, 0.25, 0.053, 0.083, \n",
    "#               0.125, 0.176, 0.222, 0.056, 0.091, 0.133, 0.182, 0.227\n",
    "distance = 0.211\n",
    "\n",
    "# Clusters with less words than this threshold are not included in visualization\n",
    "min_cluster_threshold = 5\n",
    "# Clusters with more words than this threshold are not included in visualization\n",
    "max_cluster_threshold = 20\n",
    "\n",
    "# Minimum threshold of occurences for a word to appear on graph\n",
    "freq_threshold = 10 \n",
    "# Word sets to look from (choices: vs_doc_lt, sm_doc_lt, lg_doc_lt)\n",
    "doc_choice = lg_doc_lt\n",
    "\n",
    "visualizeClusterCompFreqData(distance, freq_threshold, doc_choice, min_cluster_threshold, max_cluster_threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
